{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import io\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS credentials and configurations\n",
    "aws_access_key_id = 'AKIATCKAN534MLFAFQ4Z'\n",
    "aws_secret_access_key = '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'\n",
    "region_name = 'US East (Ohio) us-east-2'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = 'tkh-nyc-energy'\n",
    "s3_file_key = 'energy_cleaned_05272024.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift_endpoint = 'your_redshift_cluster_endpoint'\n",
    "redshift_dbname = 'your_database_name'\n",
    "redshift_user = 'your_redshift_username'\n",
    "redshift_password = 'your_redshift_password'\n",
    "redshift_port = 5439\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "\n",
    "# Function to extract data from S3\n",
    "def extract_from_s3(bucket_name, file_key):\n",
    "    response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
    "    return df\n",
    "\n",
    "# Function to transform data\n",
    "def transform_data(df):\n",
    "    # Example transformation: Convert column names to lowercase\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "    # Example transformation: Fill missing values\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to load data into Redshift\n",
    "def load_to_redshift(df, redshift_table):\n",
    "    # Establish Redshift connection\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=redshift_dbname,\n",
    "        user=redshift_user,\n",
    "        password=redshift_password,\n",
    "        host=redshift_endpoint,\n",
    "        port=redshift_port\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Create table if not exists (Example schema, adjust as necessary)\n",
    "    create_table_query = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {redshift_table} (\n",
    "        col1 TYPE,\n",
    "        col2 TYPE,\n",
    "        ...\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Load data into Redshift\n",
    "    for index, row in df.iterrows():\n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {redshift_table} (col1, col2, ...)\n",
    "        VALUES ({row['col1']}, {row['col2']}, ...);\n",
    "        \"\"\"\n",
    "        cur.execute(insert_query)\n",
    "    \n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "# ETL process\n",
    "def etl_process():\n",
    "    # Extract\n",
    "    df = extract_from_s3(s3_bucket_name, s3_file_key)\n",
    "    \n",
    "    # Transform\n",
    "    df_transformed = transform_data(df)\n",
    "    \n",
    "    # Load\n",
    "    load_to_redshift(df_transformed, 'your_redshift_table_name')\n",
    "\n",
    "# Run ETL process\n",
    "if __name__ == \"__main__\":\n",
    "    etl_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phase1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
